--- 
layout: post
title: Machine Learning Ex4 - Logistic Regression and Newton's Method
categories: 
- r
- machinelearning
- regressionanalysis
- statistics
---







<p>
<a href="http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex4/ex4.html">Exercise 4</a> is all about using Newton's Method to implement logistic regression on a classification problem.
</p>
<p>
For all this to make sense i suggest having a look at <a href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning">Andrew Ng machine learning lectures</a> on <a href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning">openclassroom</a>.
</p>
<p>
We start with a dataset representing 40 students who were admitted to college and 40 students who were not admitted, and their corresponding grades for 2 exams.
<i>Your mission, should you decide to accept it</i> is to build a binary classification model that estimates college admission chances based on a student's scores on two exams(test1 and test2).
</p>


<script src="http://www.mathjax.org/mathjax/MathJax.js" type="text/javascript">
    MathJax.Hub.Config({
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "left",
        displayIndent: "2em",
 
        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
</script>



<div class="outline-2" id="outline-container-1">
<h2 id="sec-1">Plot the Data </h2>
<div class="outline-text-2" id="text-1">


<p>
We start by looking at the data.
</p>



<pre class="src src-R"><span style="color: #b22222;"># </span><span style="color: #b22222;">reading data
</span>mydata = read.csv(<span style="color: #8b2252;">"http://spreadsheets.google.com/pub?key=0AnypY27pPCJydC1vRVEzM1VJQnNneFo5dWNzR1F5Umc&amp;output=csv"</span>, header = <span style="color: #228b22;">TRUE</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">plots
</span>plot(mydata$test1[mydata$admitted == 0], mydata$test2[mydata$admitted == 0], xlab=<span style="color: #8b2252;">"test1"</span>, ylab=<span style="color: #8b2252;">"test2"</span>, , col=<span style="color: #8b2252;">"red"</span>)
points(mydata$test1[mydata$admitted == 1], mydata$test2[mydata$admitted == 1], col=<span style="color: #8b2252;">"blue"</span>, pch=2)
legend(<span style="color: #8b2252;">"bottomright"</span>, c(<span style="color: #8b2252;">"not admitted"</span>, <span style="color: #8b2252;">"admitted"</span>), pch=c(1, 2), col=c(<span style="color: #8b2252;">"red"</span>, <span style="color: #8b2252;">"blue"</span>) )
</pre>




<p>
<img src="http://al3xandr3.github.com/img/ml-ex4-plotdata.png" alt="http://al3xandr3.github.com/img/ml-ex4-plotdata.png" />
</p>
</div>

</div>

<div class="outline-2" id="outline-container-2">
<h2 id="sec-2">A Bit of Theory </h2>
<div class="outline-text-2" id="text-2">


<p>
Most of the ideas explored in linear regression apply in same way, first we define what the hypothesis equation looks like(the mathematical representation of this knowledge). 
It originates from the line equation, but has now evolved into a new equation that returns values between [0,1] suited for binary classification. That is, we made up an equation that given test1 value and test2 value, will return the probability that the student will be admitted(y=1) into college:
</p>


\[
h_\theta(x) = g(\theta^T x) = \frac{1}{ 1 + e ^{- \theta^T x} }
\]

<p>
g is the sigmoid function. And this returns:
</p>


\[
h_\theta(x) = P (y=1 | x; \theta)
\]

<p>
Now we need to find the \(\theta\) parameters for getting a working hypothesis equation. 
To help with that search we define a cost equation, that for a given \(\theta\) returns how far off we are compared to the sample data.
</p>


\[
J(\theta) = \frac{1}{m} \sum_{i=1}^m ((-y)log(h_\theta(x)) - (1 - y) log(1- h_\theta(x)) ) 
\]

<p>
The lower the cost the better(closer to real data we get). Thus, the goal becomes to minimize the cost.
</p>
<p>
We can use <a href="http://en.wikipedia.org/wiki/File:NewtonIteration_Ani.gif">Newton's method</a> for that. Newton's method, similarly to gradient descent, is a way to search for the 0(minimum) of the derivative of the cost function. And after doing some math, the iterative \(\theta\) updates using Newton's method is defined as:
</p>


\[
\theta^{(t+1)} = \theta^{(t)} -  H^{-1} \nabla_{\theta}J 
\]

<p>
And the gradient and Hessian are defined like so(in vectorized versions):
</p>


\[
\nabla_{\theta}J  = \frac{1}{m} \sum_{i=1}^m (h_\theta(x) - y) x
\]

\[
H = \frac{1}{m} \sum_{i=1}^m [h_\theta(x) (1 - h_\theta(x)) x^T x]
\]

</div>

</div>

<div class="outline-2" id="outline-container-3">
<h2 id="sec-3">Implementation </h2>
<div class="outline-text-2" id="text-3">


<p>
First we implement the above equations:
</p>



<pre class="src src-R"><span style="color: #b22222;"># </span><span style="color: #b22222;">sigmoid
</span>g = <span style="color: #7f007f;">function</span> (z) {
  <span style="color: #7f007f;">return</span> (1 / (1 + exp(-z) ))
} <span style="color: #b22222;"># </span><span style="color: #b22222;">plot(g(c(1,2,3,4,5,6)))
</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">hypothesis 
</span>h = <span style="color: #7f007f;">function</span> (x,th) {
  <span style="color: #7f007f;">return</span>( g(x %*% th) )
} <span style="color: #b22222;"># </span><span style="color: #b22222;">h(x,th)
</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">cost
</span>J = <span style="color: #7f007f;">function</span> (x,y,th,m) {
  <span style="color: #7f007f;">return</span>( 1/m * sum(-y * log(h(x,th)) - (1 - y) * log(1 - h(x,th))) )
} <span style="color: #b22222;"># </span><span style="color: #b22222;">J(x,y,th,m)
</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">derivative of J (gradient)
</span>grad = <span style="color: #7f007f;">function</span> (x,y,th,m) {
  <span style="color: #7f007f;">return</span>( 1/m * t(x) %*% (h(x,th) - y))
} <span style="color: #b22222;"># </span><span style="color: #b22222;">grad(x,y,th,m)
</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">Hessian
</span>H = <span style="color: #7f007f;">function</span> (x,y,th,m) {
  <span style="color: #7f007f;">return</span> (1/m * t(x) %*% x * diag(h(x,th)) * diag(1 - h(x,th)))
} <span style="color: #b22222;"># </span><span style="color: #b22222;">H(x,y,th,m)
</span></pre>




<p>
Make it go (iterate until convergence):
</p>



<pre class="src src-R"><span style="color: #b22222;"># </span><span style="color: #b22222;">setup variables
</span>j = array(0,c(10,1))
m = length(mydata$test1)
x = matrix(c(rep(1,m), mydata$test1, mydata$test2), ncol=3)
y = matrix(mydata$admitted, ncol=1)
th = matrix(0,3)

<span style="color: #b22222;"># </span><span style="color: #b22222;">iterate 
</span><span style="color: #b22222;"># </span><span style="color: #b22222;">note that the newton's method converges fast, 10x is enough
</span><span style="color: #7f007f;">for</span> (i <span style="color: #7f007f;">in</span> 1:10) {
  j[i] = J(x,y,th,m) <span style="color: #b22222;"># </span><span style="color: #b22222;">stores each iteration Cost
</span>  th = th - solve(H(x,y,th,m)) %*% grad(x,y,th,m) 
}
</pre>




<p>
Have a look at the cost function by iteration:
</p>



<pre class="src src-R">plot(j, xlab=<span style="color: #8b2252;">"iterations"</span>, ylab=<span style="color: #8b2252;">"cost J"</span>)
</pre>




<p>
<img src="http://al3xandr3.github.com/img/ml-ex4-j.png" alt="http://al3xandr3.github.com/img/ml-ex4-j.png" />
</p>
<p>
See that the number of iterations needed is only 4-5, converges much faster than gradient descent.
</p>
<p>
Exercise questions:
</p>



<pre class="src src-R"><span style="color: #b22222;"># </span><span style="color: #b22222;">1. What values of  did you get? How many iterations were required for convergence?
</span>print(<span style="color: #8b2252;">"1."</span>)
print(th)

<span style="color: #b22222;"># </span><span style="color: #b22222;">2. What is the probability that a student with a score of 20 on Exam 1
</span><span style="color: #b22222;"># </span><span style="color: #b22222;">and a score of 80 on Exam 2 will not be admitted?
</span>print(<span style="color: #8b2252;">"2."</span>)
print((1 - g(c(1, 20, 80) %*% th))* 100)
</pre>



<pre class="example">
[1] "1."
            [,1]
[1,] -16.4469479
[2,]   0.1457278
[3,]   0.1618285
[1] "2."
         [,1]
[1,] 64.24722
</pre>



<p>
To visualize the fit, an important remark is that: \(P(y=1 | x ;\theta) = 0.5\) that happens when \(\theta^T x = 0\), so:
</p>



<pre class="src src-R"><span style="color: #b22222;"># </span><span style="color: #b22222;">when ax0 + bx2 + cx3 = 0 is the middle(decision boundary line),
</span><span style="color: #b22222;"># </span><span style="color: #b22222;">so given x1 from sample data, solving to x2, we get:
</span>x2 = (-1/th[3,]) * ((th[2,] * x1) + th[1,])

<span style="color: #b22222;"># </span><span style="color: #b22222;">get 2 points (that will define a line)
</span>x1 = c(min(x[,2]), max(x[,2]))

<span style="color: #b22222;"># </span><span style="color: #b22222;">plot
</span>plot(x1,x2, type=<span style="color: #8b2252;">'l'</span>,  xlab=<span style="color: #8b2252;">"test1"</span>, ylab=<span style="color: #8b2252;">"test2"</span>)
points(mydata$test1[mydata$admitted == 0], mydata$test2[mydata$admitted == 0], col=<span style="color: #8b2252;">"red"</span>)
points(mydata$test1[mydata$admitted == 1], mydata$test2[mydata$admitted == 1], col=<span style="color: #8b2252;">"blue"</span>, pch=2)
legend(<span style="color: #8b2252;">"bottomright"</span>, c(<span style="color: #8b2252;">"not admitted"</span>, <span style="color: #8b2252;">"admitted"</span>), pch=c(1, 2), col=c(<span style="color: #8b2252;">"red"</span>, <span style="color: #8b2252;">"blue"</span>) )
</pre>




<p>
<img src="http://al3xandr3.github.com/img/ml-ex4-fit.png" alt="http://al3xandr3.github.com/img/ml-ex4-fit.png" />
</p>
<p>
Beautiful.
</p>


</div>

<div class="outline-4" id="outline-container-3_1">
<h4 id="sec-3_1">Notes &amp; References: </h4>
<div class="outline-text-4" id="text-3_1">


<ul>
<li>Thanks for Tal Galili patience in adding my half-baked blog into the <a href="http://www.r-bloggers.com/">R-bloggers.com</a> list. <a href="http://www.r-bloggers.com/">Go have a peek, R-bloggers</a> is a great source of R information.
</li>
<li><a href="http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex4/ex4.html">Exercise 4 here</a>
</li>
<li><a href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning">Lectures here</a>
</li>
</ul>
</div>
</div>
</div>
<div id="postamble">

</div>

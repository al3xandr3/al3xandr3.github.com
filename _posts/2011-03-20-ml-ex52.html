--- 
layout: post
title: Machine Learning Ex5.2 - Regularized Logistic Regression
categories: 
- r
- machinelearning
- statistics
---







<p>
<a href="http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex5/ex5.html">Exercise 5.2</a> Improves the Logistic Regression implementation done in <a href="http://al3xandr3.github.com/2011/03/16/ml-ex4.html">Exercise 4</a> by adding a regularization parameter that reduces the problem of over-fitting. We will be using Newton's Method.
</p>
<p>
With implementation in R.
</p>

<div class="outline-2" id="outline-container-1">
<h2 id="sec-1">Data </h2>
<div class="outline-text-2" id="text-1">


<p>
Here's how the data we want to fit, looks like:
</p>


<script src="http://www.mathjax.org/mathjax/MathJax.js" type="text/javascript">
    MathJax.Hub.Config({
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "left",
        displayIndent: "2em",
 
        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
</script>



<pre class="src src-R"><span style="color: #b22222;"># </span><span style="color: #b22222;">linear regression
</span><span style="color: #b22222;"># </span><span style="color: #b22222;">load the data
</span>mydata = read.csv(<span style="color: #8b2252;">"http://spreadsheets.google.com/pub?key=0AnypY27pPCJydHZPN2pFbkZGd1RKeU81OFY3ZHJldWc&amp;output=csv"</span>, header = <span style="color: #228b22;">TRUE</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">plot the data
</span>plot(mydata$u[mydata$y == 0], mydata$v[mydata$y == 0],, xlab=<span style="color: #8b2252;">"u"</span>, ylab=<span style="color: #8b2252;">"v"</span>)
points(mydata$u[mydata$y == 1], mydata$v[mydata$y == 1], col=<span style="color: #8b2252;">"blue"</span>, pch=3)
legend(<span style="color: #8b2252;">"topright"</span>, c(<span style="color: #8b2252;">"y=0"</span>,<span style="color: #8b2252;">"y=1"</span>), pch=c(1, 3), col=c(<span style="color: #8b2252;">"black"</span>, <span style="color: #8b2252;">"blue"</span>), bty=<span style="color: #8b2252;">"n"</span>)
</pre>




<p>
<img src="http://al3xandr3.github.com/img/ml-ex52-data.png" alt="http://al3xandr3.github.com/img/ml-ex52-data.png" />
</p>
<p>
The idea of "fitting" is to create a mathematical model, that will separate the circles from the crosses in the plot above by learning from the existing data. That will then allow to make predictions for a new u and v value, the probability of being a cross.
</p>
</div>

</div>

<div class="outline-2" id="outline-container-2">
<h2 id="sec-2">Theory </h2>
<div class="outline-text-2" id="text-2">


<p>
Hypothesis is:
</p>


\[
h_\theta(x) = g(\theta^T x) = \frac{1}{ 1 + e ^{- \theta^T x} }
\]

<p>
Regularization is all about loosen up the tight fit, avoiding over-fitting and thus obtain a more generalized fit, that more likely will work better on new data(for doing predictions).
</p>
<p>
For that we define the cost function, with an added generalization parameter that blunts the fit, like so:
</p>


\[
J(\theta) = \frac{1}{m} \sum_{i=1}^m [(-y)log(h_\theta(x)) - (1 - y) log(1- h_\theta(x))] + \frac{\lambda}{2m} \sum_{i=1}^n \theta^2]
\]

\(\lambda\) is called the regularization parameter.

<p>
The iterative \(\theta\) updates using Newton's method is defined as:
</p>


\[
\theta^{(t+1)} = \theta^{(t)} -  H^{-1} \nabla_{\theta}J 
\]

<p>
And the gradient and Hessian are defined like so(in vectorized versions):
</p>


\[
\nabla_{\theta}J  = \frac{1}{m} \sum_{i=1}^m (h_\theta(x) - y) x + \frac{\lambda}{m} \theta
\] 

\[
H = \frac{1}{m} \sum_{i=1}^m [h_\theta(x) (1 - h_\theta(x)) x^T x] + \frac{\lambda}{m} \begin{bmatrix}
       0 & & & \\ & 1 & & \\ & & ... & \\ & & & 1
     \end{bmatrix}
\]

</div>

</div>

<div class="outline-2" id="outline-container-3">
<h2 id="sec-3">Implementation </h2>
<div class="outline-text-2" id="text-3">


<p>
Lets first define the functions above, with the added generalization parameter:
</p>



<pre class="src src-R"><span style="color: #b22222;"># </span><span style="color: #b22222;">sigmoid function
</span>g = <span style="color: #7f007f;">function</span> (z) {
  <span style="color: #7f007f;">return</span> (1 / (1 + exp(-z)))
} <span style="color: #b22222;"># </span><span style="color: #b22222;">plot(g(c(1,2,3,4,5,6)), type="l")
</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">build hight order feature vector
</span><span style="color: #b22222;"># </span><span style="color: #b22222;">for 2 features, for a given degree
</span>hi.features = <span style="color: #7f007f;">function</span> (f1,f2,deg) {
  n = ncol(f1)
  ma = matrix(rep(1,length(f1)))
  <span style="color: #7f007f;">for</span> (i <span style="color: #7f007f;">in</span> 1:deg) {
    <span style="color: #7f007f;">for</span> (j <span style="color: #7f007f;">in</span> 0:i)    
      ma = cbind(ma, f1^(i-j) * f2^j)
  }
  <span style="color: #7f007f;">return</span>(ma)
} <span style="color: #b22222;"># </span><span style="color: #b22222;">hi.features(c(1,2), c(3,4),2)
</span><span style="color: #b22222;"># </span><span style="color: #b22222;">creates: 1 u v u^2 uv v^2 ...
</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">hypothesis
</span>h = <span style="color: #7f007f;">function</span> (x,th) {
  <span style="color: #7f007f;">return</span>(g(x %*% th))
} <span style="color: #b22222;"># </span><span style="color: #b22222;">h(x,th)
</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">derivative of J 
</span>grad = <span style="color: #7f007f;">function</span> (x,y,th,m,la) {
  G = (la/m * th)
  G[1,] = 0
  <span style="color: #7f007f;">return</span>((1/m * t(x) %*% (h(x,th) - y)) +  G)
} <span style="color: #b22222;"># </span><span style="color: #b22222;">grad(x,y,th,m,la)
</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">hessian
</span>H = <span style="color: #7f007f;">function</span> (x,y,th,m,la) {
  n = length(th)
  L = la/m * diag(n)
  L[1,] = 0
  <span style="color: #7f007f;">return</span>((1/m * t(x) %*% x * diag(h(x,th)) * diag(1 - h(x,th))) + L)
} <span style="color: #b22222;"># </span><span style="color: #b22222;">H(x,y,th,m,la)
</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">cost function
</span>J = <span style="color: #7f007f;">function</span> (x,y,th,m,la) {
  pt = th
  pt[1] = 0
  A = (la/(2*m))* t(pt) %*% pt
  <span style="color: #7f007f;">return</span>((1/m * sum(-y * log(h(x,th)) - (1 - y) * log(1 - h(x,th)))) + A)
} <span style="color: #b22222;"># </span><span style="color: #b22222;">J(x,y,th,m,la)
</span></pre>




<p>
Now we can make it iterate until convergence, first for \(\lambda=1\)
</p>



<pre class="src src-R"><span style="color: #b22222;"># </span><span style="color: #b22222;">setup variables
</span>m = length(mydata$u) <span style="color: #b22222;"># </span><span style="color: #b22222;">samples
</span>x = hi.features(mydata$u, mydata$v,6)
n = ncol(x) <span style="color: #b22222;"># </span><span style="color: #b22222;">features
</span>y = matrix(mydata$y, ncol=1)

<span style="color: #b22222;"># </span><span style="color: #b22222;">lambda = 1
</span><span style="color: #b22222;"># </span><span style="color: #b22222;">use the cost function to check is works
</span>th1 = matrix(0,n)
la = 1
jiter = array(0,c(15,1))
<span style="color: #7f007f;">for</span> (i <span style="color: #7f007f;">in</span> 1:15) {
  jiter[i] = J(x,y,th1,m,la)
  th1 = th1 - solve(H(x,y,th1,m,la)) %*% grad(x,y,th1,m,la) 
}
</pre>




<p>
Validate that is converging properly, by plotting the Cost(J) function against the number of iterations.
</p>



<pre class="src src-R"><span style="color: #b22222;"># </span><span style="color: #b22222;">check that is converging correctly
</span>plot(jiter, xlab=<span style="color: #8b2252;">"iterations"</span>, ylab=<span style="color: #8b2252;">"cost J"</span>)
</pre>




<p>
<img src="http://al3xandr3.github.com/img/ml-ex52-j.png" alt="http://al3xandr3.github.com/img/ml-ex52-j.png" />
</p>
<p>
Converging well and fast, as is typical from Newton's method.
</p>
<p>
And now we make it iterate for \(\lambda=0\) and \(\lambda=10\) for comparing fits later:
</p>



<pre class="src src-R"><span style="color: #b22222;"># </span><span style="color: #b22222;">lambda = 0
</span>th0 = matrix(0,n)
la = 0
<span style="color: #7f007f;">for</span> (i <span style="color: #7f007f;">in</span> 1:15) {
  th0 = th0 - solve(H(x,y,th0,m,la)) %*% grad(x,y,th0,m,la) 
}

<span style="color: #b22222;"># </span><span style="color: #b22222;">lambda = 10
</span>th10 = matrix(0,n)
la = 10
<span style="color: #7f007f;">for</span> (i <span style="color: #7f007f;">in</span> 1:15) {
  th10 = th10 - solve(H(x,y,th10,m,la)) %*% grad(x,y,th10,m,la) 
}
</pre>




<p>
Finally calculate the decision boundary line and visualize it:
</p>



<pre class="src src-R"><span style="color: #b22222;"># </span><span style="color: #b22222;">calculate the decision boundary line
</span><span style="color: #b22222;"># </span><span style="color: #b22222;">create many points
</span>u = seq(-1, 1.2, len=200);
v = seq(-1, 1.2, len=200);
z0 = matrix(0, length(u), length(v))
z1 = matrix(0, length(u), length(v))
z10 = matrix(0, length(u), length(v))
<span style="color: #7f007f;">for</span> (i <span style="color: #7f007f;">in</span> 1:length(u)) {
  <span style="color: #7f007f;">for</span> (j <span style="color: #7f007f;">in</span> 1:length(v)) {
    z0[j,i] =  hi.features(u[i],v[j],6) %*% th0
    z1[j,i] =  hi.features(u[i],v[j],6) %*% th1
    z10[j,i] =  hi.features(u[i],v[j],6) %*% th10
  }
}

<span style="color: #b22222;"># </span><span style="color: #b22222;">plots
</span>contour(u,v,z0,nlev = 0, xlab=<span style="color: #8b2252;">"u"</span>, ylab=<span style="color: #8b2252;">"v"</span>, nlevels=0, col=<span style="color: #8b2252;">"black"</span>,lty=2)
contour(u,v,z1,nlev = 0, xlab=<span style="color: #8b2252;">"u"</span>, ylab=<span style="color: #8b2252;">"v"</span>, nlevels=0, col=<span style="color: #8b2252;">"red"</span>,lty=2, add=<span style="color: #228b22;">TRUE</span>)
contour(u,v,z10,nlev = 0, xlab=<span style="color: #8b2252;">"u"</span>, ylab=<span style="color: #8b2252;">"v"</span>, nlevels=0, col=<span style="color: #8b2252;">"green3"</span>,lty=2, add=<span style="color: #228b22;">TRUE</span>)
points(mydata$u[mydata$y == 0], mydata$v[mydata$y == 0])
points(mydata$u[mydata$y == 1], mydata$v[mydata$y == 1], col=<span style="color: #8b2252;">"blue"</span>, pch=3)
legend(<span style="color: #8b2252;">"topright"</span>,  c(expression(lambda==0), expression(lambda==1),expression(lambda==10)), lty=1, col=c(<span style="color: #8b2252;">"black"</span>, <span style="color: #8b2252;">"red"</span>,<span style="color: #8b2252;">"green3"</span>),bty=<span style="color: #8b2252;">"n"</span> )
</pre>




<p>
<img src="http://al3xandr3.github.com/img/ml-ex52-fit.png" alt="http://al3xandr3.github.com/img/ml-ex52-fit.png" />
</p>
<p>
See that the black line (\(\lambda=0\)) is the more tightly fit to the crosses, and as we increase the lambda values it becomes more loose(and more generalized) and consequently a better predictor for new unseen data.
</p>


</div>

<div class="outline-4" id="outline-container-3_1">
<h4 id="sec-3_1">References </h4>
<div class="outline-text-4" id="text-3_1">

<ul>
<li>Thanks to Andrew Ng and <a href="http://openclassroom.stanford.edu/MainFolder/HomePage.php">OpenClassRoom</a> for the great lessons. 
</li>
</ul>

</div>
</div>
</div>

